{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projekt końcowy z Metod Numerycznych 2023/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytania:\n",
    "- Czy można do raportu użyć markdowna? Można\n",
    "- O co chodzi z upewnieniem się czy wszystkie dane są w tym samym formacie i jednostkach? \n",
    "Napisać, że z danej strony wartości są w tych jednostkach i w tych jednostkach. Nie trzeba w żaden sposób sprawdzać\n",
    "- Analiza wyników???\n",
    "- Analizować z całego dnia dane\n",
    "- Można uśrednić z tygodnia, zrobić kwartał\n",
    "- Jesli kilkuletnie to z miesiąca\n",
    "- On wsm poleca kwartał\n",
    "- Można z suwakiem mapkę\n",
    "- Temperatura i jakies 3 inne, sumarycznie 4 wartości\n",
    "- Prezentacja na 10 minut (muszą być slajdy, nie program,mapki, wykresy, podsumowania, tabele)\n",
    "- Dane uzupełnić, jak coś to samemu sobie usunąć pomiary, wtedy będą rozwiązania dokładne\n",
    "- mówi o aproksymacji\n",
    "- wymagane są 4 stacje, powinno być z conajmniej kwartału"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zebranie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Użyto danych z biblioteki meteostat. Wybrano 5 lokalizacji: \n",
    "- Kraków\n",
    "- Warszawa\n",
    "- Wrocław\n",
    "- Szczecin\n",
    "- Gdańsk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "from meteostat import Point, Daily, units\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = {\n",
    "    'Kraków': Point(50.0647, 19.9450),\n",
    "    'Warszawa': Point(52.2297, 21.0122),\n",
    "    'Wrocław': Point(51.1079, 17.0385),\n",
    "    'Szczecin': Point(53.4285, 14.5528),\n",
    "    'Gdańsk': Point(54.3520, 18.6466)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dane pobrano z pierwszego kwartału 2024 roku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2024, 1, 1)\n",
    "end = datetime(2024, 3, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plik CSV dla Kraków został pomyślnie wygenerowany jako kraków_weather_data.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plik CSV dla Warszawa został pomyślnie wygenerowany jako warszawa_weather_data.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "Warning: Cannot load daily/12425.csv.gz from https://bulk.meteostat.net/v2/\n",
      "FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plik CSV dla Wrocław został pomyślnie wygenerowany jako wrocław_weather_data.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plik CSV dla Szczecin został pomyślnie wygenerowany jako szczecin_weather_data.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plik CSV dla Gdańsk został pomyślnie wygenerowany jako gdańsk_weather_data.csv.\n"
     ]
    }
   ],
   "source": [
    "dataframes = []\n",
    "for city, point in cities.items():\n",
    "    data = Daily(point, start, end)\n",
    "    data = data.fetch()\n",
    "    data['City'] = city\n",
    "    filename = f'{city.lower()}_weather_data.csv'\n",
    "    data.to_csv(filename, index=True)\n",
    "    print(f\"Plik CSV dla {city} został pomyślnie wygenerowany jako {filename}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dane zostały wygenerowane, zatem można przystąpić do obróbki danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>snow</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>wpgt</th>\n",
       "      <th>pres</th>\n",
       "      <th>tsun</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>260.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>35.2</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kraków</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>193.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>29.6</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kraków</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>7.4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>223.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.5</td>\n",
       "      <td>997.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kraków</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>6.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>9.2</td>\n",
       "      <td>7.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>235.0</td>\n",
       "      <td>25.5</td>\n",
       "      <td>53.7</td>\n",
       "      <td>998.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kraków</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>5.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>46.3</td>\n",
       "      <td>1009.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kraków</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time  tavg  tmin  tmax  prcp  snow   wdir  wspd  wpgt    pres  tsun  \\\n",
       "0  2024-01-01   2.7   0.5   4.7   8.9   NaN  260.0  12.8  35.2  1009.2   NaN   \n",
       "1  2024-01-02   4.9   2.9   7.0   9.9   NaN  193.0   9.7  29.6  1008.0   NaN   \n",
       "2  2024-01-03   7.4   4.1  10.2   5.5   NaN  223.0  19.2  44.5   997.5   NaN   \n",
       "3  2024-01-04   6.6   3.1   9.2   7.1   NaN  235.0  25.5  53.7   998.6   NaN   \n",
       "4  2024-01-05   1.3   0.0   6.5   5.8   NaN   25.0  14.4  46.3  1009.8   NaN   \n",
       "\n",
       "     City  \n",
       "0  Kraków  \n",
       "1  Kraków  \n",
       "2  Kraków  \n",
       "3  Kraków  \n",
       "4  Kraków  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('kraków_weather_data.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tym datasecie występuje 10 zmiennych dotyczących pogody. Ograniczmy go do 4: temperatury, ciśnienia atmosferycznego, sumy opadów i prędkości wiatru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Należy usunąć pozostałe kolumny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>tavg</th>\n",
       "      <th>prcp</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>2.7</td>\n",
       "      <td>8.9</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>Kraków</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>4.9</td>\n",
       "      <td>9.9</td>\n",
       "      <td>9.7</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>Kraków</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>7.4</td>\n",
       "      <td>5.5</td>\n",
       "      <td>19.2</td>\n",
       "      <td>997.5</td>\n",
       "      <td>Kraków</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>6.6</td>\n",
       "      <td>7.1</td>\n",
       "      <td>25.5</td>\n",
       "      <td>998.6</td>\n",
       "      <td>Kraków</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>1.3</td>\n",
       "      <td>5.8</td>\n",
       "      <td>14.4</td>\n",
       "      <td>1009.8</td>\n",
       "      <td>Kraków</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time  tavg  prcp  wspd    pres    City\n",
       "0  2024-01-01   2.7   8.9  12.8  1009.2  Kraków\n",
       "1  2024-01-02   4.9   9.9   9.7  1008.0  Kraków\n",
       "2  2024-01-03   7.4   5.5  19.2   997.5  Kraków\n",
       "3  2024-01-04   6.6   7.1  25.5   998.6  Kraków\n",
       "4  2024-01-05   1.3   5.8  14.4  1009.8  Kraków"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['tmin', 'tmax', 'snow', 'wdir', 'wpgt', 'tsun'], axis = 1)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tavg - średnia temperatura powietrza [°C]\n",
    "- wspd - średnia prędkość wiatru [km/h]\n",
    "- pres - średnie ciśnienie atmosferyczne [hPa]\n",
    "- prcp - średnia suma opadów [mm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 91 entries, 0 to 90\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   time    91 non-null     object \n",
      " 1   tavg    91 non-null     float64\n",
      " 2   prcp    91 non-null     float64\n",
      " 3   wspd    91 non-null     float64\n",
      " 4   pres    91 non-null     float64\n",
      " 5   City    91 non-null     object \n",
      "dtypes: float64(4), object(2)\n",
      "memory usage: 4.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wszystkie dane, którymi będziemy się posługiwać, są w tym samym formacie (float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mamy do przeanalizowania łącznie 91 wierszy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time    0\n",
       "tavg    0\n",
       "prcp    0\n",
       "wspd    0\n",
       "pres    0\n",
       "City    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W datasecie nie występują braki, dlatego aby móc użyć metod numerycznych i sprawdzić ich efektywność, należy niektóre wartości usunąć "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>tavg</th>\n",
       "      <th>prcp</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>2.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>Kraków</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>4.9</td>\n",
       "      <td>9.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>Kraków</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>997.5</td>\n",
       "      <td>Kraków</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.1</td>\n",
       "      <td>25.5</td>\n",
       "      <td>998.6</td>\n",
       "      <td>Kraków</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>1.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.4</td>\n",
       "      <td>1009.8</td>\n",
       "      <td>Kraków</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time  tavg  prcp  wspd    pres    City\n",
       "0  2024-01-01   2.7   NaN  12.8  1009.2  Kraków\n",
       "1  2024-01-02   4.9   9.9   NaN  1008.0  Kraków\n",
       "2  2024-01-03   NaN   5.5   NaN   997.5  Kraków\n",
       "3  2024-01-04   NaN   7.1  25.5   998.6  Kraków\n",
       "4  2024-01-05   1.3   NaN  14.4  1009.8  Kraków"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def randomly_remove_values(df, frac, columns_to_nan):\n",
    "    total_elements = df[columns_to_nan].size\n",
    "    num_elements_to_remove = int(total_elements * frac)\n",
    "    indices = [(row, df.columns.get_loc(col)) for row in range(df.shape[0]) for col in columns_to_nan]\n",
    "    indices_to_remove = np.random.choice(range(len(indices)), num_elements_to_remove, replace=False)\n",
    "\n",
    "    for index in indices_to_remove:\n",
    "        row, col = indices[index]\n",
    "        df.iat[row, col] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "columns_to_nan = ['tavg', 'prcp', 'wspd', 'pres']\n",
    "df_with_nan = randomly_remove_values(df.copy(), 0.2, columns_to_nan)\n",
    "df_with_nan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time     0\n",
       "tavg    19\n",
       "prcp    18\n",
       "wspd    22\n",
       "pres    13\n",
       "City     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_nan.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obliczanie brakujących danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uzupełnijmy braki za pomocą metod numerycznych. W tym przypadku najlepsza będzie aproksymacja, ponieważ nie chcemy, żeby funkcje przechodziły dokładnie przez wszystkie anomalie, ale żeby zwracały uśrednioną wartość"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'snow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\szewc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'snow'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m             df\u001b[38;5;241m.\u001b[39mloc[i, column_name] \u001b[38;5;241m=\u001b[39m horner_interpolation(coeffs[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], i)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m---> 21\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mapproximate_missing_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msnow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLiczba brakujących wartości po aproksymacji:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum())\n",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m, in \u001b[0;36mapproximate_missing_values\u001b[1;34m(df, column_name, degree)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapproximate_missing_values\u001b[39m(df, column_name, degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m      8\u001b[0m     indices \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m----> 9\u001b[0m     not_nan_indices \u001b[38;5;241m=\u001b[39m df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mnotna()]\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     10\u001b[0m     x_data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[not_nan_indices]\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     11\u001b[0m     y_data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[not_nan_indices, column_name]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\szewc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\szewc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'snow'"
     ]
    }
   ],
   "source": [
    "def horner_interpolation(coeffs, x):\n",
    "    result = coeffs[0]\n",
    "    for i in range(1, len(coeffs)):\n",
    "        result = result * x + coeffs[i]\n",
    "    return result\n",
    "\n",
    "def approximate_missing_values(df, column_name, degree=3):\n",
    "    indices = df.index.tolist()\n",
    "    not_nan_indices = df[df[column_name].notna()].index.tolist()\n",
    "    x_data = df.loc[not_nan_indices].index.tolist()\n",
    "    y_data = df.loc[not_nan_indices, column_name].tolist()\n",
    "    \n",
    "    coeffs = np.polyfit(x_data, y_data, degree)\n",
    "    \n",
    "    for i in indices:\n",
    "        if np.isnan(df.loc[i, column_name]):\n",
    "            df.loc[i, column_name] = horner_interpolation(coeffs[::-1], i)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = approximate_missing_values(df, 'snow', degree=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
